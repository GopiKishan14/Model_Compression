# Model_Compression
Accumulation of work done in internship at IISc. See report [here](https://github.com/GopiKishan14/Model_Compression/blob/master/InternSummary.pdf)

## Flow
* Read about Knowledge Distillation(KD) and AutoML.
* Implemented KD and verified results.
* Read about applying KD on object detection tasks.
* Struggled with paper **Generative Knowledge Distillation for General Purpose Function Compression**
* Read about LSTM, Convolutional LSTM, ConvLSTM.
* Read MCnet paper and went through the code base.
* Read about Encoders & Decoders.
* Read AutoEncoders and implemented in TF-Keras.
* Compressed the model using KD and observed an interesting fact.
* Read VAE & GAN.
* Implemented VAE in TF & keras.
* Proposed improvement in vanilla VAE : Dirichlet VAE. [Later found to be already exiting].

## In parallel
* Paprticipated in MicroNet challenge.
* Read about pruning and quantization.
* Proposped weight matrix factorization using SVD techniques to reduce complexity from O(n^2) to O(n). [Later found to be already exiting].
* Read paper MobilenetV1 and MobilenetV2.
* Implemented on CIFAR 100 dataset.

## Self Works.
* Completed STAT110 course by Prof. Blitzstein from Harvard.
* Reading book Bishop's Pattern Recognition and Machine Learning. 
